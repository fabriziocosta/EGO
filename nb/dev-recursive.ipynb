{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-Chemical-Data\" data-toc-modified-id=\"Import-Chemical-Data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import Chemical Data</a></span></li><li><span><a href=\"#EGO-utilities\" data-toc-modified-id=\"EGO-utilities-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>EGO utilities</a></span></li><li><span><a href=\"#Examples\" data-toc-modified-id=\"Examples-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Examples</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EGO decompositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T18:18:30.960011Z",
     "start_time": "2019-03-18T18:18:30.350842Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style><style>.output_png {display: table-cell;text-align: center;vertical-align: middle;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import networkx as nx\n",
    "import toolz as tz\n",
    "from eden.util import configure_logging\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "configure_logging(logger, verbosity=1)\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import HTML\n",
    "HTML('<style>.container { width:95% !important; }</style><style>.output_png {display: table-cell;text-align: center;vertical-align: middle;}</style>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Chemical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T18:18:31.198619Z",
     "start_time": "2019-03-18T18:18:30.961449Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from toolz import curry, pipe\n",
    "from eden_chem.io.pubchem import download\n",
    "from eden_chem.io.rdkitutils import sdf_to_nx\n",
    "\n",
    "download_active = curry(download)(active=True)\n",
    "download_inactive = curry(download)(active=False)\n",
    "\n",
    "def get_pos_graphs(assay_id): return pipe(assay_id, download_active, sdf_to_nx, list)\n",
    "def get_neg_graphs(assay_id): return pipe(assay_id, download_inactive, sdf_to_nx, list)\n",
    "\n",
    "from constrActive.utils import pre_process, _random_sample\n",
    "from eden_chem.io.pubchem import get_assay_description\n",
    "\n",
    "def make_data(assay_id, max_size=20):\n",
    "    logger.debug('_'*80)\n",
    "    logger.debug('Dataset %s info:'%assay_id)\n",
    "    desc = get_assay_description(assay_id)\n",
    "    logging.debug('\\n%s'%desc)\n",
    "    # extract pos and neg graphs\n",
    "    all_pos_graphs, all_neg_graphs = get_pos_graphs(assay_id), get_neg_graphs(assay_id)\n",
    "    # remove too large and too small graphs and outliers\n",
    "    initial_max_size=2000\n",
    "    initial_max_size=max(initial_max_size,max_size)\n",
    "    args=dict(initial_max_size=initial_max_size, fraction_to_remove=.1, n_neighbors_for_outliers=9, remove_similar=False, max_size=max_size)\n",
    "    logging.debug('\\nPositive graphs')\n",
    "    pos_graphs = pre_process(all_pos_graphs, **args)\n",
    "    logging.debug('\\nNegative graphs')\n",
    "    neg_graphs = pre_process(all_neg_graphs, **args)\n",
    "    logger.debug('-'*80)\n",
    "    return pos_graphs, neg_graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EGO utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T18:18:31.232537Z",
     "start_time": "2019-03-18T18:18:31.200227Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ego.vectorize import set_feature_size\n",
    "from ego.encode import make_encoder\n",
    "from ego.decompose import concatenate, compose\n",
    "\n",
    "from ego.path import decompose_path\n",
    "from ego.cycle_basis import decompose_cycles_and_non_cycles, decompose_non_cycles, decompose_cycles\n",
    "from ego.iterated_clique import decompose_iterated_clique\n",
    "from ego.context import decompose_context\n",
    "from ego.dilatate import decompose_dilatate\n",
    "from ego.paired_neighborhoods import decompose_paired_neighborhoods, decompose_neighborhood\n",
    "from ego.centrality import decompose_central_and_non_central, decompose_central, decompose_non_central\n",
    "from ego.discriminative import decompose_discriminative_and_non_discriminative, decompose_discriminative, decompose_non_discriminative\n",
    "\n",
    "\n",
    "from ego.abstract_label import preprocess_abstract_label\n",
    "from ego.minor_graph import preprocess_minor_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-18T18:18:30.336Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from GraphDesign.utils import draw_mols\n",
    "from collections import defaultdict\n",
    "\n",
    "def select_unique(codes, fragments):\n",
    "    already_seen = set()\n",
    "    unique_codes=[]\n",
    "    unique_fragments=[]\n",
    "    code_counts = defaultdict(int)\n",
    "    for code, fragment in zip(codes, fragments):\n",
    "        if code not in already_seen:\n",
    "            unique_codes.append(code)\n",
    "            unique_fragments.append(fragment)\n",
    "            already_seen.add(code)\n",
    "        code_counts[code] += 1\n",
    "    return unique_codes, unique_fragments, code_counts\n",
    "\n",
    "\n",
    "def show_decomposition_graphs(graphs, decompose_funcs, preprocessors=None, nbits=14):\n",
    "    feature_size, bitmask = set_feature_size(nbits=nbits)\n",
    "    encoding_func = make_encoder(decompose_funcs, preprocessors=preprocessors, bitmask=bitmask, seed=1)\n",
    "    for g in graphs:\n",
    "        print('_'*120)\n",
    "        draw_mols([g])\n",
    "        codes, fragments = encoding_func(g)\n",
    "        unique_codes, unique_fragments, code_counts = select_unique(codes, fragments)\n",
    "        titles = ['%d   #%d'%(id,code_counts[id]) for id in unique_codes]\n",
    "        #titles = list(map(str, unique_codes))\n",
    "        print('%d unique components in %d fragments'%(len(unique_codes),len(codes)))\n",
    "        if unique_fragments:\n",
    "            draw_mols(unique_fragments, titles, n_graphs_per_line=9)\n",
    "        else:\n",
    "            print('No fragments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-18T18:18:30.341Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Dataset 624249 info:\n",
      "\n",
      "qHTS screen for small molecules that inhibit ELG1-dependent DNA repair in human embryonic kidney (HEK293T) cells expressing luciferase-tagged ELG1: Hit Confirmation using MMS Stimulated ELG1\n",
      "Reading from file: PUBCHEM/AID624249_active.sdf\n",
      "Reading from file: PUBCHEM/AID624249_inactive.sdf\n",
      "\n",
      "Positive graphs\n",
      "original size:355\n",
      "random sample:355\n",
      "removed disconnected:339\n",
      "size filter:322\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "assay_ids = ['624249']\n",
    "assay_id=assay_ids[0]\n",
    "configure_logging(logger, verbosity=2)\n",
    "pos_graphs, neg_graphs = make_data(assay_id, max_size=300)\n",
    "configure_logging(logger, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-18T18:18:30.343Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ego.predictor import Classifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def make_data_split(pos_graphs, neg_graphs, test_frac=.3):\n",
    "    test_set_size = int(len(pos_graphs)*test_frac)\n",
    "    test_pos_graphs = pos_graphs[:test_set_size]\n",
    "    train_pos_graphs = pos_graphs[test_set_size:]\n",
    "    test_set_size = int(len(neg_graphs)*test_frac)\n",
    "    test_neg_graphs = neg_graphs[:test_set_size]\n",
    "    train_neg_graphs = neg_graphs[test_set_size:]\n",
    "    train_graphs=train_pos_graphs+train_neg_graphs\n",
    "    train_target=[1]*len(train_pos_graphs)+[-1]*len(train_neg_graphs)\n",
    "    test_graphs=test_pos_graphs+test_neg_graphs\n",
    "    test_target=[1]*len(test_pos_graphs)+[-1]*len(test_neg_graphs)\n",
    "    return train_graphs, train_target, test_graphs, test_target\n",
    "\n",
    "def eval_EDeN(pos_graphs, neg_graphs):\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    from eden.graph import vectorize as eden_vectorize\n",
    "    train_graphs, train_target, test_graphs, test_target = make_data_split(pos_graphs, neg_graphs, test_frac=.3)\n",
    "    train_x = eden_vectorize(train_graphs, r=3,d=6)\n",
    "    test_x = eden_vectorize(test_graphs, r=3,d=6)\n",
    "    estimator = SGDClassifier(penalty='elasticnet', tol=1e-3)\n",
    "    estimator.fit(train_x, train_target)\n",
    "    preds = estimator.decision_function(test_x)\n",
    "    return roc_auc_score(test_target, preds)\n",
    "\n",
    "def eval_order0(pos_graphs, neg_graphs, decompose_func):\n",
    "    train_graphs, train_target, test_graphs, test_target = make_data_split(pos_graphs, neg_graphs, test_frac=.3)\n",
    "    disc = Classifier(decompose_func=decompose_func, preprocessor=None, nbits=14)\n",
    "    disc.fit(train_graphs, train_target)\n",
    "    preds = disc.decision_function(test_graphs)\n",
    "    return roc_auc_score(test_target, preds)\n",
    "\n",
    "from ego.learn import make_node_scoring_func\n",
    "def eval_order1(pos_graphs, neg_graphs, decompose_func):\n",
    "    train_graphs, train_target, test_graphs, test_target = make_data_split(pos_graphs, neg_graphs, test_frac=.3)\n",
    "    node_scoring_func = make_node_scoring_func(train_graphs, train_target, decomposition_funcs=decompose_func)\n",
    "    \n",
    "    f1 = decompose_discriminative(min_size=3, max_size=30, node_scoring_func=node_scoring_func, threshold=0.2)\n",
    "    f2 = decompose_discriminative(min_size=3, max_size=30, node_scoring_func=node_scoring_func, threshold=0.4)\n",
    "    f3 = decompose_discriminative(min_size=3, max_size=30, node_scoring_func=node_scoring_func, threshold=0.8)\n",
    "    g1 = compose(decompose_iterated_clique(n_iter=2),decompose_cycles_and_non_cycles,f1)\n",
    "    g2 = compose(decompose_iterated_clique(n_iter=2),decompose_cycles_and_non_cycles,f2)\n",
    "    g3 = compose(decompose_iterated_clique(n_iter=2),decompose_cycles_and_non_cycles,f3)\n",
    "    decompose_func2 = concatenate(g1,g2,g3)\n",
    "\n",
    "    disc = Classifier(decompose_func=[decompose_func, decompose_func2], preprocessor=None, nbits=14)\n",
    "    disc.fit(train_graphs, train_target)\n",
    "    preds = disc.decision_function(test_graphs)\n",
    "    return roc_auc_score(test_target, preds)\n",
    "\n",
    "def eval_order2(pos_graphs, neg_graphs, decompose_func):\n",
    "    train_graphs, train_target, test_graphs, test_target = make_data_split(pos_graphs, neg_graphs, test_frac=.3)\n",
    "    node_scoring_func = make_node_scoring_func(train_graphs, train_target, decomposition_funcs=decompose_func)\n",
    "    f1 = decompose_discriminative(min_size=3, max_size=30, node_scoring_func=node_scoring_func, threshold=0.2)\n",
    "    f2 = decompose_discriminative(min_size=3, max_size=30, node_scoring_func=node_scoring_func, threshold=0.4)\n",
    "    f3 = decompose_discriminative(min_size=3, max_size=30, node_scoring_func=node_scoring_func, threshold=0.8)\n",
    "    g1 = compose(decompose_iterated_clique(n_iter=2),decompose_cycles_and_non_cycles,f1)\n",
    "    g2 = compose(decompose_iterated_clique(n_iter=2),decompose_cycles_and_non_cycles,f2)\n",
    "    g3 = compose(decompose_iterated_clique(n_iter=2),decompose_cycles_and_non_cycles,f3)\n",
    "    decompose_func2 = concatenate(g1,g2,g3)\n",
    "    node_scoring_func2 = make_node_scoring_func(train_graphs, train_target, decomposition_funcs=decompose_func2)\n",
    "    f12 = decompose_discriminative(min_size=3, max_size=30, node_scoring_func=node_scoring_func2, threshold=0.2)\n",
    "    f22 = decompose_discriminative(min_size=3, max_size=30, node_scoring_func=node_scoring_func2, threshold=0.4)\n",
    "    f32 = decompose_discriminative(min_size=3, max_size=30, node_scoring_func=node_scoring_func2, threshold=0.8)\n",
    "    g12 = compose(decompose_iterated_clique(n_iter=2),decompose_cycles_and_non_cycles,f12)\n",
    "    g22 = compose(decompose_iterated_clique(n_iter=2),decompose_cycles_and_non_cycles,f22)\n",
    "    g32 = compose(decompose_iterated_clique(n_iter=2),decompose_cycles_and_non_cycles,f32)\n",
    "    decompose_func3 = concatenate(g12,g22,g32)\n",
    "    \n",
    "    disc = Classifier(decompose_func=[decompose_func, decompose_func2, decompose_func3], preprocessor=None, nbits=14)\n",
    "    disc.fit(train_graphs, train_target)\n",
    "    preds = disc.decision_function(test_graphs)\n",
    "    return roc_auc_score(test_target, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-18T18:18:30.347Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1 = decompose_neighborhood(radius=2)\n",
    "f2 = decompose_paired_neighborhoods(radius=1, distance=4)\n",
    "f3 = compose(\n",
    "    decompose_iterated_clique(n_iter=1),\n",
    "    decompose_cycles_and_non_cycles)\n",
    "decompose_func = concatenate(f1,f2, f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-18T18:18:30.372Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "assay_ids = ['624249']\n",
    "for assay_id in assay_ids:\n",
    "    configure_logging(logger, verbosity=2)\n",
    "    pos_graphs, neg_graphs = make_data(assay_id, max_size=1000)\n",
    "    configure_logging(logger, verbosity=1)\n",
    "    \n",
    "    try:\n",
    "        %time print('EDeN AUC %.3f' % eval_EDeN(pos_graphs, neg_graphs))\n",
    "        %time print('L0 AUC %.3f' % eval_order0(pos_graphs, neg_graphs, decompose_func))\n",
    "        %time print('L1 AUC %.3f' % eval_order1(pos_graphs, neg_graphs, decompose_func))\n",
    "        %time print('L2 AUC %.3f' % eval_order2(pos_graphs, neg_graphs, decompose_func))\n",
    "        print\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-18T18:18:30.375Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graphs = pos_graphs + neg_graphs\n",
    "targets = np.array([1]*len(pos_graphs) + [-1]*len(neg_graphs)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-18T18:18:30.378Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(pos_graphs)\n",
    "gs = pos_graphs[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-18T18:18:30.382Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_decomposition_graphs(gs, decompose_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-18T18:18:30.385Z"
    }
   },
   "outputs": [],
   "source": [
    "from ego.learn import make_node_scoring_func\n",
    "node_scoring_func = make_node_scoring_func(graphs, targets, decomposition_funcs=decompose_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-18T18:18:30.388Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "decompose_func = decompose_discriminative(min_size=3, max_size=30, node_scoring_func=node_scoring_func, threshold=0.5)\n",
    "show_decomposition_graphs(gs, decompose_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-18T18:18:30.391Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "decompose_func = decompose_non_discriminative(min_size=3, max_size=30, node_scoring_func=node_scoring_func, threshold=0.5)\n",
    "show_decomposition_graphs(gs, decompose_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-18T18:18:30.427Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f1 = decompose_discriminative(min_size=3, max_size=30, node_scoring_func=node_scoring_func, threshold=0.2)\n",
    "f2 = decompose_discriminative(min_size=3, max_size=30, node_scoring_func=node_scoring_func, threshold=0.4)\n",
    "f3 = decompose_discriminative(min_size=3, max_size=30, node_scoring_func=node_scoring_func, threshold=0.8)\n",
    "decompose_func = concatenate(f1,f2,f3)\n",
    "show_decomposition_graphs(gs, decompose_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-18T18:18:30.432Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f1 = decompose_discriminative(min_size=3, max_size=30, node_scoring_func=node_scoring_func, threshold=0.2)\n",
    "f2 = decompose_discriminative(min_size=3, max_size=30, node_scoring_func=node_scoring_func, threshold=0.4)\n",
    "f3 = decompose_discriminative(min_size=3, max_size=30, node_scoring_func=node_scoring_func, threshold=0.8)\n",
    "decompose_func = compose(\n",
    "    decompose_iterated_clique(n_iter=2),\n",
    "    decompose_cycles_and_non_cycles,\n",
    "    concatenate(f1,f2,f3))\n",
    "show_decomposition_graphs(gs, decompose_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-18T18:18:30.434Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "decompose_func2 = compose(\n",
    "    decompose_iterated_clique(n_iter=1),\n",
    "    decompose_cycles_and_non_cycles,\n",
    "    decompose_discriminative(min_size=3, max_size=30, node_scoring_func=node_scoring_func, threshold=0.5))\n",
    "show_decomposition_graphs(gs, decompose_func2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-18T18:18:30.437Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ego.learn import make_node_scoring_func\n",
    "node_scoring_func2 = make_node_scoring_func(graphs, targets, decomposition_funcs=decompose_func2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-18T18:18:30.441Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "decompose_func3 = compose(\n",
    "    decompose_iterated_clique(n_iter=1),\n",
    "    decompose_cycles_and_non_cycles,\n",
    "    decompose_discriminative(min_size=3, max_size=50, node_scoring_func=node_scoring_func2, threshold=0.5))\n",
    "show_decomposition_graphs(gs, decompose_func3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
